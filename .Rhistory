distance = "glm",
estimand = "ATT",
method = "nearest",
datasets = imp_data)
cobalt::bal.tab(matched_data)
#run and combined pooled models
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened + (1|post)))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1]
))
#conduct matching
matched_data <- weightthem(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt) + (1|post),
approach = "within",
method = "glm",
estimand = "ATT",
datasets = imp_data)
#conduct matching
matched_data <- weightthem(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt) + factor(post),
approach = "within",
method = "glm",
estimand = "ATT",
datasets = imp_data)
#according to "reporting results" in this vignette (https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html)
#diffs below 0.1 indicate good balancing
cobalt::bal.tab(matched_data)
#run and combined pooled models
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened + (1|post)))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1]
))
?matchit
#conduct matching
matched_data <- matchthem(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt),
approach = "within",
exact = ~ post,
distance = "glm",
estimand = "ATT",
method = "nearest",
datasets = imp_data)
cobalt::bal.tab(matched_data)
#run and combined pooled models
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened + (1|post)))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened + (1|post), weights = weights))), conf.int = TRUE)[2, -1]
))
#run and combined pooled models
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened))), conf.int = TRUE)[2, -1]
))
?weightit
#conduct matching
matched_data <- weightthem(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt) + factor(post),
approach = "within",
method = "glm",
estimand = "ATT",
datasets = imp_data)
#according to "reporting results" in this vignette (https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html)
#diffs below 0.1 indicate good balancing
cobalt::bal.tab(matched_data)
#run and combined pooled models (weights automatically added)
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened))), conf.int = TRUE)[2, -1]
))
#check variance inflation factor with one of the imputed datasets
#indicate that we should drop the toxicity metric
car::vif(glm(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_tox) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt), data = mice::complete(imp_data), family = "binomial"), family = logistic)
car::vif(glm(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt), data = mice::complete(imp_data), family = "binomial"), family = logistic)
summary(glm(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt), data = mice::complete(imp_data))
)
glm(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt), data = mice::complete(imp_data))
summary(glm(intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt), data = mice::complete(imp_data)))
#conduct matching
matched_data <- weightthem(intervened ~ scale(likes) + scale(order),
#intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_tox) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt) + factor(post),
approach = "within",
method = "glm",
estimand = "ATT",
datasets = imp_data)
#according to "reporting results" in this vignette (https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html)
#diffs below 0.1 indicate good balancing
cobalt::bal.tab(matched_data)
#run and combined pooled models (weights automatically added)
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened))), conf.int = TRUE)[2, -1]
))
#conduct matching
matched_data <- weightthem(intervened ~ scale(likes) + scale(order) + (1|post),
#intervened ~ scale(likes) + scale(nchar) + scale(order) + scale(vdr_comp) + scale(prsp_tox) + scale(prsp_id_atk) + scale(prsp_ins) + scale(prsp_prof) + scale(prsp_thrt) + factor(post),
approach = "within",
method = "glm",
estimand = "ATT",
datasets = imp_data)
aic(glm(intervened ~ scale(likes) + scale(order)), glm(intervened ~ scale(likes) + scale(order) + (1|post)))
AIC(glm(intervened ~ scale(likes) + scale(order)), glm(intervened ~ scale(likes) + scale(order) + (1|post)))
AIC(glm(intervened ~ scale(likes) + scale(order), data = mice::complete(imp_data)), glm(intervened ~ scale(likes) + scale(order) + (1|post), data = mice::complete(imp_data)))
AIC(glm(intervened ~ scale(likes) + scale(order), data = mice::complete(imp_data)), glm(intervened ~ scale(likes) + scale(order) + (1|factor(post)), data = mice::complete(imp_data)))
AIC(glm(intervened ~ scale(likes) + scale(order), data = mice::complete(imp_data)), glm(intervened ~ scale(likes) + scale(order) + (1|post), data = mice::complete(imp_data)))
pool(with(matched_data, lm(scale(n_replies) ~ intervened)))
pool(with(matched_data, lm(intervened ~ scale(n_replies))))
pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies))))
summary(pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies)))))
car::vif(pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies)))))
car::vif(pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies)))))
summary(pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies)))))
summary(pool(with(matched_data, lm(intervened ~ scale(n_replies) + scale(med_likes_replies) + scale(med_comp_replies) + scale(med_id_atk_replies) + scale(med_prof_replies) + scale(med_thrt_replies) + scale(med_nchar_replies)))))
#run and combined pooled models (weights automatically added)
do.call(rbind, list(
summary(pool(with(matched_data, lm(scale(n_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_likes_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_comp_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_id_atk_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_prof_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_thrt_replies) ~ intervened))), conf.int = TRUE)[2, -1],
summary(pool(with(matched_data, lm(scale(med_nchar_replies) ~ intervened))), conf.int = TRUE)[2, -1]
))
#set working directory and load data
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
load("data/priors_and_simulations.RData")
#store number of simulations
n_sim <- nrow(priors_and_simulations[[1]])
#store max and min values for normalization before inference
sim_max <- max(unlist(priors_and_simulations[[2]]))
sim_min <- 0
#load in regiolect data
regiolects <- read.csv("data/regiolect_distributions.csv")
colnames(regiolects) <- gsub("X", "", colnames(regiolects))
#identify unique types
types <- sort(unique(regiolects$type))
#format simulations for bayesflow
formatted_simulations <- lapply(1:n_sim, function(x){
lapply(1:4, function(y){
a <- sort(priors_and_simulations[[2]][[x]][[1]][[y]], decreasing = TRUE)
b <- sort(priors_and_simulations[[2]][[x]][[2]][[y]], decreasing = TRUE)
a <- (a - sim_min)/(sim_max - sim_min)
b <- (b - sim_min)/(sim_max - sim_min)
return(list(a, b))
})
})
#format observed data to match simulated data during inference
formatted_obs <- lapply(1:4, function(y){
if(y == 1){reg <- "west"}
if(y == 2){reg <- "east"}
if(y == 3){reg <- "newf"}
if(y == 4){reg <- "nova"}
lapply(c(2005, 2019), function(x){
temp <- regiolects$n[which(regiolects$regio == reg & regiolects$year == x)]
temp <- (temp - sim_min)/(sim_max - sim_min)
return(c(temp, rep(0, length(types) - length(temp))))
})
})
#correct column names and scale
colnames(priors_and_simulations[[1]])[1] <- c("n_init")
scaled_priors <- scale(priors_and_simulations[[1]])
#restructure priors so it's compatible with json format
priors <- lapply(1:n_sim, function(x){as.numeric(scaled_priors[x, ])})
#save data for bayesflow
train_data <- list(priors[1:99000], formatted_simulations[1:99000])
test_data <- list(priors[99001:100000], formatted_simulations[99001:100000])
jsonlite::write_json(train_data, "04_bayesflow_analysis/train_data.json")
#set working directory and load data
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
load("data/priors_and_simulations.RData")
#store number of simulations
n_sim <- nrow(priors_and_simulations[[1]])
#store max and min values for normalization before inference
sim_max <- max(unlist(priors_and_simulations[[2]]))
sim_min <- 0
#load in regiolect data
regiolects <- read.csv("data/regiolect_distributions.csv")
colnames(regiolects) <- gsub("X", "", colnames(regiolects))
#identify unique types
types <- sort(unique(regiolects$type))
#format simulations for bayesflow
formatted_simulations <- lapply(1:n_sim, function(x){
lapply(1:4, function(y){
a <- sort(priors_and_simulations[[2]][[x]][[1]][[y]], decreasing = TRUE)
b <- sort(priors_and_simulations[[2]][[x]][[2]][[y]], decreasing = TRUE)
a <- (a - sim_min)/(sim_max - sim_min)
b <- (b - sim_min)/(sim_max - sim_min)
return(list(a, b))
})
})
#format observed data to match simulated data during inference
formatted_obs <- lapply(1:4, function(y){
if(y == 1){reg <- "west"}
if(y == 2){reg <- "east"}
if(y == 3){reg <- "newf"}
if(y == 4){reg <- "nova"}
lapply(c(2005, 2019), function(x){
temp <- regiolects$n[which(regiolects$regio == reg & regiolects$year == x)]
temp <- (temp - sim_min)/(sim_max - sim_min)
return(c(temp, rep(0, length(types) - length(temp))))
})
})
#correct column names and scale
colnames(priors_and_simulations[[1]])[1] <- c("n_init")
scaled_priors <- scale(priors_and_simulations[[1]])
#restructure priors so it's compatible with json format
priors <- lapply(1:n_sim, function(x){as.numeric(scaled_priors[x, ])})
length(priors)
#save data for bayesflow
train_data <- list(priors[1:49000], formatted_simulations[1:49000])
test_data <- list(priors[49001:50000], formatted_simulations[49001:50000])
jsonlite::write_json(train_data, "04_bayesflow_analysis/train_data.json")
jsonlite::write_json(test_data, "04_bayesflow_analysis/test_data.json")
jsonlite::write_json(list(formatted_obs), "04_bayesflow_analysis/obs_data.json")
#read in training and validation loss
loss <- read.csv("04_bayesflow_analysis/training_loss.csv")
loss <- loss$Loss
valid <- read.csv("04_bayesflow_analysis/validation_loss.csv")
valid <- valid$Loss
#training loss is raw while validation loss is the final value for each epoch, so divide raw loss values into the 200 epochs for averaging
to_avg <- length(loss)/300
#compute median training loss for each epoch and restructure for plotting
med_loss <- sapply(1:300, function(x){
inds <- ((to_avg*(x-1))+1):(to_avg*x)
median(loss[inds])
})
plot_data <- data.frame(loss = med_loss, valid = valid, epoch = c(1:300))
#training loss is raw while validation loss is the final value for each epoch, so divide raw loss values into the 200 epochs for averaging
to_avg <- length(loss)/500
#compute median training loss for each epoch and restructure for plotting
med_loss <- sapply(1:500, function(x){
inds <- ((to_avg*(x-1))+1):(to_avg*x)
median(loss[inds])
})
plot_data <- data.frame(loss = med_loss, valid = valid, epoch = c(1:500))
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = loss), color = "black") +
geom_line(aes(y = valid), color = "gray60") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
library(ggplot2)
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = loss), color = "black") +
geom_line(aes(y = valid), color = "gray60") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
#write function for plotting
plot_abc <- function(posts, param, xlim, xlab){
#specify probability distributions for each parameter
if(param == "n_init"){
param <- 1
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 5000, max = 50000)
}
if(param == "mu"){
param <- 2
prior <- dbeta(seq(xlim[1], xlim[2], length.out = 2^10), 1, 40)
}
if(param == "dems"){
param <- 3
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 2, max = 5)
}
if(param == "frequency"){
param <- 4
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 1, sd = 0.2)
}
if(param == "content"){
param <- 5
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 0, sd = 2)
}
#structure data for plotting
data <- data.frame(param = c(seq(xlim[1], xlim[2], length.out = 2^10),
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$x),
density = c(prior,
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$y),
group = factor(c(rep(0, 2^10), rep(1, 2^10))))
#create partial ggplot
temp <- ggplot(data = data, aes(x = param, y = density, group = group)) +
geom_line(aes(linetype = group, color = group)) +
scale_x_continuous(expand = c(0, 0), breaks = seq(from = xlim[1], to = xlim[2], length.out = 4)) +
scale_color_manual(values = c("grey", "black")) +
scale_linetype_manual(values = c("solid", "solid")) +
xlab(xlab) + ylab("Density") + theme_linedraw() +
theme(legend.position = "none", plot.margin = margin(t = 5, r = 15, b = 5, l = 5),
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.y = element_text(angle = 90), axis.title.y = element_text(vjust = -4))
#add the relevant y axis labels for plotting
if(param %in% c(1, 2)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = scales::scientific_format(digits = 2))
}
if(param %in% c(4, 5)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = round(max(data$density), digits = 1))
}
#return object
return(temp)
}
#create plot for each parameter to be included in main panel
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
#read in training and validation loss
loss <- read.csv("04_bayesflow_analysis/training_loss.csv")
loss <- loss$Loss
valid <- read.csv("04_bayesflow_analysis/validation_loss.csv")
valid <- valid$Loss
#training loss is raw while validation loss is the final value for each epoch, so divide raw loss values into the 200 epochs for averaging
to_avg <- length(loss)/100
#compute median training loss for each epoch and restructure for plotting
med_loss <- sapply(1:100, function(x){
inds <- ((to_avg*(x-1))+1):(to_avg*x)
median(loss[inds])
})
plot_data <- data.frame(loss = med_loss, valid = valid, epoch = c(1:100))
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = loss), color = "black") +
geom_line(aes(y = valid), color = "gray60") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
#write function for plotting
plot_abc <- function(posts, param, xlim, xlab){
#specify probability distributions for each parameter
if(param == "n_init"){
param <- 1
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 5000, max = 50000)
}
if(param == "mu"){
param <- 2
prior <- dbeta(seq(xlim[1], xlim[2], length.out = 2^10), 1, 40)
}
if(param == "dems"){
param <- 3
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 2, max = 5)
}
if(param == "frequency"){
param <- 4
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 1, sd = 0.2)
}
if(param == "content"){
param <- 5
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 0, sd = 2)
}
#structure data for plotting
data <- data.frame(param = c(seq(xlim[1], xlim[2], length.out = 2^10),
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$x),
density = c(prior,
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$y),
group = factor(c(rep(0, 2^10), rep(1, 2^10))))
#create partial ggplot
temp <- ggplot(data = data, aes(x = param, y = density, group = group)) +
geom_line(aes(linetype = group, color = group)) +
scale_x_continuous(expand = c(0, 0), breaks = seq(from = xlim[1], to = xlim[2], length.out = 4)) +
scale_color_manual(values = c("grey", "black")) +
scale_linetype_manual(values = c("solid", "solid")) +
xlab(xlab) + ylab("Density") + theme_linedraw() +
theme(legend.position = "none", plot.margin = margin(t = 5, r = 15, b = 5, l = 5),
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.y = element_text(angle = 90), axis.title.y = element_text(vjust = -4))
#add the relevant y axis labels for plotting
if(param %in% c(1, 2)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = scales::scientific_format(digits = 2))
}
if(param %in% c(4, 5)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = round(max(data$density), digits = 1))
}
#return object
return(temp)
}
# PLOT POSTERIORS ---------------------------------------------------------
#create plot for each parameter to be included in main panel
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
#write function for plotting
plot_abc <- function(posts, param, xlim, xlab){
#specify probability distributions for each parameter
if(param == "n_init"){
param <- 1
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 5000, max = 50000)
}
if(param == "mu"){
param <- 2
prior <- dbeta(seq(xlim[1], xlim[2], length.out = 2^10), 1, 40)
}
if(param == "dems"){
param <- 3
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 2, max = 5)
}
if(param == "frequency"){
param <- 4
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 1, sd = 0.2)
}
if(param == "content"){
param <- 5
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 0, sd = 2)
}
#structure data for plotting
data <- data.frame(param = c(seq(xlim[1], xlim[2], length.out = 2^10),
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$x),
density = c(prior,
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$y),
group = factor(c(rep(0, 2^10), rep(1, 2^10))))
#create partial ggplot
temp <- ggplot(data = data, aes(x = param, y = density, group = group)) +
geom_line(aes(linetype = group, color = group)) +
scale_x_continuous(expand = c(0, 0), breaks = seq(from = xlim[1], to = xlim[2], length.out = 4)) +
scale_color_manual(values = c("grey", "black")) +
scale_linetype_manual(values = c("solid", "solid")) +
xlab(xlab) + ylab("Density") + theme_linedraw() +
theme(legend.position = "none", plot.margin = margin(t = 5, r = 15, b = 5, l = 5),
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.y = element_text(angle = 90), axis.title.y = element_text(vjust = -4))
#add the relevant y axis labels for plotting
if(param %in% c(1, 2)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = scales::scientific_format(digits = 2))
}
if(param %in% c(4, 5)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = round(max(data$density), digits = 1))
}
#return object
return(temp)
}
#create plot for each parameter to be included in main panel
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
