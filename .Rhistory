table$signif[which(table$p.value < 0.001)] <- "***"
table$p.value <- format(table$p.value, scientific = TRUE, digits = 2)
colnames(table) <- c("Predictor", "Est.", "2.5%", "97.5%", "p", " ")
#print table
flex_table <- flextable(table) %>% theme_vanilla() %>% set_table_properties(layout = "autofit") %>%
set_caption("The average treatment effect on the treated (ATT) for each predictor in the weighted model.")
flex_table
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
#write function for plotting
plot_abc <- function(posts, param, xlim, xlab){
#specify probability distributions for each parameter
if(param == "n_init"){
param <- 1
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 5000, max = 50000)
}
if(param == "mu"){
param <- 2
prior <- dbeta(seq(xlim[1], xlim[2], length.out = 2^10), 1, 40)
}
if(param == "dems"){
param <- 3
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 2, max = 5)
}
if(param == "frequency"){
param <- 4
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 1, sd = 0.2)
}
if(param == "content"){
param <- 5
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 0, sd = 2)
}
#structure data for plotting
data <- data.frame(param = c(seq(xlim[1], xlim[2], length.out = 2^10),
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$x),
density = c(prior,
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$y),
group = factor(c(rep(0, 2^10), rep(1, 2^10))))
#create partial ggplot
temp <- ggplot(data = data, aes(x = param, y = density, group = group)) +
geom_line(aes(linetype = group, color = group)) +
scale_x_continuous(expand = c(0, 0), breaks = seq(from = xlim[1], to = xlim[2], length.out = 4)) +
scale_color_manual(values = c("grey", "black")) +
scale_linetype_manual(values = c("solid", "solid")) +
xlab(xlab) + ylab("Density") + theme_linedraw() +
theme(legend.position = "none", plot.margin = margin(t = 5, r = 15, b = 5, l = 5),
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.y = element_text(angle = 90), axis.title.y = element_text(vjust = -4))
#add the relevant y axis labels for plotting
if(param %in% c(1, 2)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = scales::scientific_format(digits = 2))
}
if(param %in% c(4, 5)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = round(max(data$density), digits = 1))
}
#return object
return(temp)
}
#create plot for each parameter to be included in main panel
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
#save figure
png(paste0("figures/main_posteriors.png"), width = 7, height = 5, units = "in", res = 600)
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
dev.off()
#read in training and validation loss
loss <- read.csv("04_bayesflow_analysis/training_loss.csv")
loss <- loss$Loss
valid <- read.csv("04_bayesflow_analysis/validation_loss.csv")
valid <- valid$Loss
#training loss is raw while validation loss is the final value for each epoch, so divide raw loss values into the 200 epochs for averaging
to_avg <- length(loss)/500
#compute median training loss for each epoch and restructure for plotting
med_loss <- sapply(1:500, function(x){
inds <- ((to_avg*(x-1))+1):(to_avg*x)
median(loss[inds])
})
plot_data <- data.frame(loss = med_loss, valid = valid, epoch = c(1:500))
#save plot of loss curves
png(paste0("figures/loss_curve.png"), width = 7, height = 3, units = "in", res = 600)
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = loss), color = "black") +
geom_line(aes(y = valid), color = "gray60") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
dev.off()
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = loss), color = "black") +
geom_line(aes(y = valid), color = "gray60") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = valid), color = "gray60") +
geom_line(aes(y = loss), color = "black") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
png(paste0("figures/loss_curve.png"), width = 7, height = 3, units = "in", res = 600)
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = valid), color = "gray60") +
geom_line(aes(y = loss), color = "black") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
dev.off()
#read in the regiolect data
data <- read.csv("data/regiolect_distributions.csv")
types <- sort(unique(data$type))
regios <- c("west", "east", "newf", "nova")
#convert to object for modeling
regio_dists <- lapply(regios, function(x){
temp <- data[which(data$year == 2019 & data$regio == x), ]
out <- temp$n[match(types, temp$type)]
out[which(is.na(out))] <- 0
out <- sort(out, decreasing = TRUE)
out <- out[which(out > 0)]
return(out)
})
#store observed values and scale priors to compute distance to estimated values
obs_vals <- c(median((posts$n_init - mean(priors_and_simulations[[1]]$n_init_west))/sd(priors_and_simulations[[1]]$n_init_west)),
median((posts$mu - mean(priors_and_simulations[[1]]$mu))/sd(priors_and_simulations[[1]]$mu)),
median((posts$dems - mean(priors_and_simulations[[1]]$dems))/sd(priors_and_simulations[[1]]$dems)),
median((posts$frequency - mean(priors_and_simulations[[1]]$frequency))/sd(priors_and_simulations[[1]]$frequency)),
median((posts$content - mean(priors_and_simulations[[1]]$content))/sd(priors_and_simulations[[1]]$content)))
scaled_priors <- scale(priors_and_simulations[[1]])
#compute distances between prior parameter values and estimated parameter values
dists <- sapply(1:nrow(scaled_priors), function(x){dist(rbind(obs_vals, scaled_priors[x, ]))})
#keep the 100 closest distributions to plot
closest <- order(dists)[1:100]
closest <- priors_and_simulations[[2]][closest]
#plot closest distributions for the west regiolect
png(paste0("figures/west_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[1]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[1]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[1]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[1]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the east regiolect
png(paste0("figures/east_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[2]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[2]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[2]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[2]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the newfoundland regiolect
png(paste0("figures/newf_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[3]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[3]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[3]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[3]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the nova scotia regiolect
png(paste0("figures/nova_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[4]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[4]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[4]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[4]], col = "red", lwd = 2)
dev.off()
#combine and save as panelled figure
png(paste0("figures/combined_posts.png"), width = 5.7, height = 4, units = "in", res = 600)
plot_grid(ggplot() + draw_image("figures/west_posts.png") + theme_void(),
ggplot() + draw_image("figures/east_posts.png") + theme_void(),
ggplot() + draw_image("figures/nova_posts.png") + theme_void(),
ggplot() + draw_image("figures/newf_posts.png") + theme_void(),
labels = c("A", "B", "C", "D"))
dev.off()
#delete individual panels
file.remove(c("figures/west_posts.png", "figures/east_posts.png", "figures/nova_posts.png", "figures/newf_posts.png"))
#set working directory and load packages
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
library(ggplot2)
library(cowplot)
load("data/priors_and_simulations.RData")
#load in posterior predictions and convert back to original scale for plotting
posts <- jsonlite::read_json("04_bayesflow_analysis/posterior_predictions.json")
posts <- data.frame(do.call(rbind, lapply(1:length(posts), function(x){unlist(posts[[x]])})))
colnames(posts) <- c("n_init", "mu", "dems", "frequency", "content")
posts$n_init <- (posts$n_init*sd(priors_and_simulations[[1]]$n_init_west)) + mean(priors_and_simulations[[1]]$n_init_west)
posts$mu <- (posts$mu*sd(priors_and_simulations[[1]]$mu)) + mean(priors_and_simulations[[1]]$mu)
posts$dems <- (posts$dems*sd(priors_and_simulations[[1]]$dems)) + mean(priors_and_simulations[[1]]$dems)
posts$frequency <- (posts$frequency*sd(priors_and_simulations[[1]]$frequency)) + mean(priors_and_simulations[[1]]$frequency)
posts$content <- (posts$content*sd(priors_and_simulations[[1]]$content)) + mean(priors_and_simulations[[1]]$content)
#write function for plotting
plot_abc <- function(posts, param, xlim, xlab){
#specify probability distributions for each parameter
if(param == "n_init"){
param <- 1
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 5000, max = 50000)
}
if(param == "mu"){
param <- 2
prior <- dbeta(seq(xlim[1], xlim[2], length.out = 2^10), 1, 40)
}
if(param == "dems"){
param <- 3
prior <- dunif(seq(xlim[1], xlim[2], length.out = 2^10), min = 2, max = 5)
}
if(param == "frequency"){
param <- 4
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 1, sd = 0.2)
}
if(param == "content"){
param <- 5
prior <- truncnorm::dtruncnorm(seq(xlim[1], xlim[2], length.out = 2^10), a = 0, mean = 0, sd = 2)
}
#structure data for plotting
data <- data.frame(param = c(seq(xlim[1], xlim[2], length.out = 2^10),
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$x),
density = c(prior,
density(posts[, param], n = 2^10, from = xlim[1], to = xlim[2])$y),
group = factor(c(rep(0, 2^10), rep(1, 2^10))))
#create partial ggplot
temp <- ggplot(data = data, aes(x = param, y = density, group = group)) +
geom_line(aes(linetype = group, color = group)) +
scale_x_continuous(expand = c(0, 0), breaks = seq(from = xlim[1], to = xlim[2], length.out = 4)) +
scale_color_manual(values = c("grey", "black")) +
scale_linetype_manual(values = c("solid", "solid")) +
xlab(xlab) + ylab("Density") + theme_linedraw() +
theme(legend.position = "none", plot.margin = margin(t = 5, r = 15, b = 5, l = 5),
panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.y = element_text(angle = 90), axis.title.y = element_text(vjust = -4))
#add the relevant y axis labels for plotting
if(param %in% c(1, 2)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = scales::scientific_format(digits = 2))
}
if(param %in% c(4, 5)){
temp <- temp + scale_y_continuous(expand = c(0, 0), breaks = c(max(data$density)), limits = c(0, max(data$density)), labels = round(max(data$density), digits = 1))
}
#return object
return(temp)
}
# PLOT POSTERIORS ---------------------------------------------------------
#create plot for each parameter to be included in main panel
n_init_plot <- plot_abc(posts, "n_init", c(5000, 50000), "Population Size")
mu_plot <- plot_abc(posts, "mu", c(0, 0.06), "Innovation Rate")
content_plot <- plot_abc(posts, "content", c(0, 3), "Content Bias")
frequency_plot <- plot_abc(posts, "frequency", c(0, 3), "Frequency Bias")
#save figure
png(paste0("figures/main_posteriors.png"), width = 7, height = 5, units = "in", res = 600)
plot_grid(n_init_plot + theme(legend.position = "none"),
mu_plot + theme(legend.position = "none"),
frequency_plot + theme(legend.position = "none") + geom_vline(xintercept = 1, linetype = "dashed"),
content_plot + theme(legend.position = "none"),
nrow = 2, labels = c("A", "B", "C", "D"))
dev.off()
# POSTERIOR SIMULATIONS ---------------------------------------------------
#read in the regiolect data
data <- read.csv("data/regiolect_distributions.csv")
types <- sort(unique(data$type))
regios <- c("west", "east", "newf", "nova")
#convert to object for modeling
regio_dists <- lapply(regios, function(x){
temp <- data[which(data$year == 2019 & data$regio == x), ]
out <- temp$n[match(types, temp$type)]
out[which(is.na(out))] <- 0
out <- sort(out, decreasing = TRUE)
out <- out[which(out > 0)]
return(out)
})
#store observed values and scale priors to compute distance to estimated values
obs_vals <- c(median((posts$n_init - mean(priors_and_simulations[[1]]$n_init_west))/sd(priors_and_simulations[[1]]$n_init_west)),
median((posts$mu - mean(priors_and_simulations[[1]]$mu))/sd(priors_and_simulations[[1]]$mu)),
median((posts$dems - mean(priors_and_simulations[[1]]$dems))/sd(priors_and_simulations[[1]]$dems)),
median((posts$frequency - mean(priors_and_simulations[[1]]$frequency))/sd(priors_and_simulations[[1]]$frequency)),
median((posts$content - mean(priors_and_simulations[[1]]$content))/sd(priors_and_simulations[[1]]$content)))
scaled_priors <- scale(priors_and_simulations[[1]])
#compute distances between prior parameter values and estimated parameter values
dists <- sapply(1:nrow(scaled_priors), function(x){dist(rbind(obs_vals, scaled_priors[x, ]))})
#keep the 100 closest distributions to plot
closest <- order(dists)[1:100]
closest <- priors_and_simulations[[2]][closest]
#plot closest distributions for the west regiolect
png(paste0("figures/west_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[1]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[1]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[1]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[1]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the east regiolect
png(paste0("figures/east_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[2]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[2]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[2]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[2]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the newfoundland regiolect
png(paste0("figures/newf_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[3]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[3]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[3]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[3]], col = "red", lwd = 2)
dev.off()
#plot closest distributions for the nova scotia regiolect
png(paste0("figures/nova_posts.png"), width = 5, height = 3.5, units = "in", res = 600)
par(mar = c(4, 4, 0.1, 0.1))
plot(regio_dists[[4]], type = "l", xlab = "Syllable Type (Ranked)", ylab = "Frequency", col = "white")
for(x in 1:length(priors_and_simulations[[2]])){
vals <- sort(priors_and_simulations[[2]][[x]][[2]][[4]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("gray88", 1))
}
for(x in 1:length(closest)){
vals <- sort(closest[[x]][[2]][[4]], decreasing = TRUE)
vals <- vals[which(vals > 0)]
lines(vals, col = scales::alpha("black", 1))
}
lines(regio_dists[[4]], col = "red", lwd = 2)
dev.off()
#combine and save as panelled figure
png(paste0("figures/combined_posts.png"), width = 5.7, height = 4, units = "in", res = 600)
plot_grid(ggplot() + draw_image("figures/west_posts.png") + theme_void(),
ggplot() + draw_image("figures/east_posts.png") + theme_void(),
ggplot() + draw_image("figures/nova_posts.png") + theme_void(),
ggplot() + draw_image("figures/newf_posts.png") + theme_void(),
labels = c("A", "B", "C", "D"))
dev.off()
#delete individual panels
file.remove(c("figures/west_posts.png", "figures/east_posts.png", "figures/nova_posts.png", "figures/newf_posts.png"))
# LOSS CURVE --------------------------------------------------------------
#read in training and validation loss
loss <- read.csv("04_bayesflow_analysis/training_loss.csv")
loss <- loss$Loss
valid <- read.csv("04_bayesflow_analysis/validation_loss.csv")
valid <- valid$Loss
#training loss is raw while validation loss is the final value for each epoch, so divide raw loss values into the 200 epochs for averaging
to_avg <- length(loss)/500
#compute median training loss for each epoch and restructure for plotting
med_loss <- sapply(1:500, function(x){
inds <- ((to_avg*(x-1))+1):(to_avg*x)
median(loss[inds])
})
plot_data <- data.frame(loss = med_loss, valid = valid, epoch = c(1:500))
#save plot of loss curves
png(paste0("figures/loss_curve.png"), width = 7, height = 3, units = "in", res = 600)
ggplot(plot_data, aes(x = epoch)) +
geom_line(aes(y = valid), color = "gray60") +
geom_line(aes(y = loss), color = "black") +
scale_x_continuous(name = "Epoch") +
scale_y_continuous(name = "Median Training Loss", sec.axis = sec_axis(~., "Validation Loss")) +
theme_linedraw() +
theme(axis.title.y.right = element_text(color = "gray60"))
dev.off()
median(posts$content)
mean(posts$content)
mean(posts$frequency)
median(posts$frequency)
mean(posts$n_init)
mean(posts$mu)
quantile(posts$mu, probs = c(0.025, 0.975))
posts$dems
round(posts$dems)
table(round(posts$dems))
as.numeric(table(round(posts$dems)))
as.numeric(table(round(posts$dems)))/nrow(posts)
setwd("~/Documents/Work/Spring 2022/Mourning Warblers/MourningWarblers")
load("data/priors_and_simulations.RData")
#store number of simulations
n_sim <- nrow(priors_and_simulations[[1]])
#store max and min values for normalization before inference
sim_max <- max(unlist(priors_and_simulations[[2]]))
sim_min <- 0
#load in regiolect data
regiolects <- read.csv("data/regiolect_distributions.csv")
colnames(regiolects) <- gsub("X", "", colnames(regiolects))
#identify unique types
types <- sort(unique(regiolects$type))
#format simulations for bayesflow
formatted_simulations <- lapply(1:n_sim, function(x){
lapply(1:4, function(y){
a <- sort(priors_and_simulations[[2]][[x]][[1]][[y]], decreasing = TRUE)
b <- sort(priors_and_simulations[[2]][[x]][[2]][[y]], decreasing = TRUE)
a <- (a - sim_min)/(sim_max - sim_min)
b <- (b - sim_min)/(sim_max - sim_min)
return(list(a, b))
})
})
#format observed data to match simulated data during inference
formatted_obs <- lapply(1:4, function(y){
if(y == 1){reg <- "west"}
if(y == 2){reg <- "east"}
if(y == 3){reg <- "newf"}
if(y == 4){reg <- "nova"}
lapply(c(2005, 2019), function(x){
temp <- regiolects$n[which(regiolects$regio == reg & regiolects$year == x)]
temp <- (temp - sim_min)/(sim_max - sim_min)
return(c(temp, rep(0, length(types) - length(temp))))
})
})
#correct column names and scale
colnames(priors_and_simulations[[1]])[1] <- c("n_init")
scaled_priors <- scale(priors_and_simulations[[1]])
#restructure priors so it's compatible with json format
priors <- lapply(1:n_sim, function(x){as.numeric(scaled_priors[x, ])})
#save data for bayesflow
train_data <- list(priors[1:95000], formatted_simulations[1:95000])
test_data <- list(priors[95001:100000], formatted_simulations[95001:100000])
jsonlite::write_json(train_data, "04_bayesflow_analysis/train_data.json")
jsonlite::write_json(test_data, "04_bayesflow_analysis/test_data.json")
jsonlite::write_json(list(formatted_obs), "04_bayesflow_analysis/obs_data.json")
